seed: 7.000000
n_iter: 30
scoring: roc_auc
-- SVM MODEL --
Number of experiments: 10
Best: 0.875000 using {'SVM__C': 7, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.975000 using {'SVM__C': 50, 'SVM__gamma': 0.01, 'SVM__kernel': 'rbf'}
Best: 0.866667 using {'SVM__C': 10, 'SVM__gamma': 0.01, 'SVM__kernel': 'rbf'}
Best: 0.950000 using {'SVM__C': 400, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.883333 using {'SVM__C': 2, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.900000 using {'SVM__C': 2, 'SVM__gamma': 0.01, 'SVM__kernel': 'rbf'}
Best: 0.908333 using {'SVM__C': 400, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.916667 using {'SVM__C': 200, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.916667 using {'SVM__C': 400, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
Best: 0.850000 using {'SVM__C': 50, 'SVM__gamma': 0.001, 'SVM__kernel': 'rbf'}
AUC = 0.8591304347826085
-- KNN MODEL --
Number of experiments: 10
Best: 0.875000 using {'KNN__n_neighbors': 9, 'KNN__weights': 'distance'}
Best: 0.958333 using {'KNN__n_neighbors': 7, 'KNN__weights': 'distance'}
Best: 0.850000 using {'KNN__n_neighbors': 11, 'KNN__weights': 'distance'}
Best: 0.966667 using {'KNN__n_neighbors': 9, 'KNN__weights': 'distance'}
Best: 0.825000 using {'KNN__n_neighbors': 7, 'KNN__weights': 'distance'}
Best: 0.866667 using {'KNN__n_neighbors': 11, 'KNN__weights': 'distance'}
Best: 0.825000 using {'KNN__n_neighbors': 7, 'KNN__weights': 'distance'}
Best: 0.850000 using {'KNN__n_neighbors': 11, 'KNN__weights': 'distance'}
Best: 0.833333 using {'KNN__n_neighbors': 11, 'KNN__weights': 'distance'}
Best: 0.808333 using {'KNN__n_neighbors': 11, 'KNN__weights': 'distance'}
AUC = 0.76
-- RF MODEL --
Number of experiments: 10
Best: 0.900000 using {'RF__max_depth': 3, 'RF__n_estimators': 200}
Best: 1.000000 using {'RF__max_depth': 2, 'RF__n_estimators': 30}
Best: 0.875000 using {'RF__max_depth': 3, 'RF__n_estimators': 30}
Best: 0.975000 using {'RF__max_depth': 3, 'RF__n_estimators': 60}
Best: 0.925000 using {'RF__max_depth': 5, 'RF__n_estimators': 10}
Best: 0.925000 using {'RF__max_depth': 2, 'RF__n_estimators': 30}
Best: 0.866667 using {'RF__max_depth': 10, 'RF__n_estimators': 400}
Best: 0.900000 using {'RF__max_depth': 1, 'RF__n_estimators': 10}
Best: 0.925000 using {'RF__max_depth': 4, 'RF__n_estimators': 10}
Best: 0.883333 using {'RF__max_depth': 3, 'RF__n_estimators': 30}
AUC = 0.8817391304347826
-- ADAB MODEL --
Number of experiments: 10
Best: 0.925000 using {'ADAB__base_estimator__max_depth': 2, 'ADAB__n_estimators': 200}
Best: 1.000000 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 60}
Best: 0.916667 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 10}
Best: 0.958333 using {'ADAB__base_estimator__max_depth': 2, 'ADAB__n_estimators': 400}
Best: 0.941667 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 100}
Best: 0.958333 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 30}
Best: 0.916667 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 60}
Best: 0.883333 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 200}
Best: 0.950000 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 400}
Best: 0.866667 using {'ADAB__base_estimator__max_depth': 1, 'ADAB__n_estimators': 400}
AUC = 0.8869565217391304
-- LGB MODEL --
Number of experiments: 10
Best: 0.500000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.500000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.500000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.500000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.500000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.637500 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.600000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 60}
Best: 0.687500 using {'LGB__max_depth': 1, 'LGB__n_estimators': 400}
Best: 0.675000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 10}
Best: 0.700000 using {'LGB__max_depth': 1, 'LGB__n_estimators': 100}
AUC = 0.8391304347826087
-- XGB MODEL --
Number of experiments: 10
Best: 0.900000 using {'XGB__max_depth': 1, 'XGB__n_estimators': 30}
Best: 0.987500 using {'XGB__max_depth': 1, 'XGB__n_estimators': 30}
Best: 0.941667 using {'XGB__max_depth': 1, 'XGB__n_estimators': 30}
Best: 1.000000 using {'XGB__max_depth': 2, 'XGB__n_estimators': 10}
Best: 0.925000 using {'XGB__max_depth': 1, 'XGB__n_estimators': 100}
Best: 0.975000 using {'XGB__max_depth': 1, 'XGB__n_estimators': 60}
Best: 0.916667 using {'XGB__max_depth': 1, 'XGB__n_estimators': 200}
Best: 0.958333 using {'XGB__max_depth': 2, 'XGB__n_estimators': 60}
Best: 0.941667 using {'XGB__max_depth': 2, 'XGB__n_estimators': 60}
Best: 0.933333 using {'XGB__max_depth': 1, 'XGB__n_estimators': 100}
AUC = 0.9321739130434783
-- DT MODEL --
Number of experiments: 10
Best: 0.779167 using {'DT__criterion': 'gini', 'DT__max_depth': 2, 'DT__splitter': 'random'}
The binary tree structure has 7 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 21] <= 0.10354250169697998 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 30] <= 0.9590794234122861 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 4] <= -1.044377664220961 else to node 6.
		node=5 is a leaf node.
		node=6 is a leaf node.
Best: 0.904167 using {'DT__criterion': 'entropy', 'DT__max_depth': 4, 'DT__splitter': 'random'}
The binary tree structure has 13 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 5] <= -0.7935966484096103 else to node 2.
	node=1 is a leaf node.
	node=2 is a split node: go to node 3 if X[:, 40] <= 0.08015248931536578 else to node 8.
		node=3 is a split node: go to node 4 if X[:, 10] <= -1.678961278665393 else to node 5.
			node=4 is a leaf node.
			node=5 is a split node: go to node 6 if X[:, 0] <= 0.3440752018343495 else to node 7.
				node=6 is a leaf node.
				node=7 is a leaf node.
		node=8 is a split node: go to node 9 if X[:, 19] <= 0.609617121964114 else to node 12.
			node=9 is a split node: go to node 10 if X[:, 25] <= 0.2552887551317835 else to node 11.
				node=10 is a leaf node.
				node=11 is a leaf node.
			node=12 is a leaf node.
Best: 0.775000 using {'DT__criterion': 'entropy', 'DT__max_depth': 3, 'DT__splitter': 'random'}
The binary tree structure has 13 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 39] <= 0.5910217671326614 else to node 8.
	node=1 is a split node: go to node 2 if X[:, 28] <= 0.009419164637376465 else to node 5.
		node=2 is a split node: go to node 3 if X[:, 34] <= 0.017030851817663728 else to node 4.
			node=3 is a leaf node.
			node=4 is a leaf node.
		node=5 is a split node: go to node 6 if X[:, 42] <= 0.46291887078261396 else to node 7.
			node=6 is a leaf node.
			node=7 is a leaf node.
	node=8 is a split node: go to node 9 if X[:, 24] <= -0.09365076118115123 else to node 10.
		node=9 is a leaf node.
		node=10 is a split node: go to node 11 if X[:, 34] <= -0.7216368461260756 else to node 12.
			node=11 is a leaf node.
			node=12 is a leaf node.
Best: 0.800000 using {'DT__criterion': 'gini', 'DT__max_depth': 2, 'DT__splitter': 'random'}
The binary tree structure has 7 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 39] <= 0.5998229639965209 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 28] <= -0.038918258756635904 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 30] <= 1.4263132727716301 else to node 6.
		node=5 is a leaf node.
		node=6 is a leaf node.
Best: 0.825000 using {'DT__criterion': 'gini', 'DT__max_depth': 3, 'DT__splitter': 'best'}
The binary tree structure has 11 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 27] <= -0.054559445939958096 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 16] <= 1.4528287649154663 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 37] <= 1.101449429988861 else to node 8.
		node=5 is a split node: go to node 6 if X[:, 37] <= -1.2257859110832214 else to node 7.
			node=6 is a leaf node.
			node=7 is a leaf node.
		node=8 is a split node: go to node 9 if X[:, 33] <= 0.18160345405340195 else to node 10.
			node=9 is a leaf node.
			node=10 is a leaf node.
Best: 0.795833 using {'DT__criterion': 'gini', 'DT__max_depth': 2, 'DT__splitter': 'random'}
The binary tree structure has 7 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 39] <= 0.5106543250840587 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 43] <= 0.673751903813911 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 30] <= 1.3859948930761097 else to node 6.
		node=5 is a leaf node.
		node=6 is a leaf node.
Best: 0.775000 using {'DT__criterion': 'entropy', 'DT__max_depth': 3, 'DT__splitter': 'best'}
The binary tree structure has 9 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 27] <= 0.10101866722106934 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 40] <= 1.8809568881988525 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 43] <= 0.2682838961482048 else to node 6.
		node=5 is a leaf node.
		node=6 is a split node: go to node 7 if X[:, 21] <= 0.8528863191604614 else to node 8.
			node=7 is a leaf node.
			node=8 is a leaf node.
Best: 0.808333 using {'DT__criterion': 'gini', 'DT__max_depth': 2, 'DT__splitter': 'random'}
The binary tree structure has 7 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 21] <= 0.08038939992509153 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 24] <= -1.4180852282592586 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 36] <= -0.5566609378585194 else to node 6.
		node=5 is a leaf node.
		node=6 is a leaf node.
Best: 0.833333 using {'DT__criterion': 'gini', 'DT__max_depth': 3, 'DT__splitter': 'random'}
The binary tree structure has 13 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 39] <= 0.5724786006048637 else to node 8.
	node=1 is a split node: go to node 2 if X[:, 28] <= -0.009397849819641202 else to node 5.
		node=2 is a split node: go to node 3 if X[:, 43] <= 0.2741616369948261 else to node 4.
			node=3 is a leaf node.
			node=4 is a leaf node.
		node=5 is a split node: go to node 6 if X[:, 5] <= -0.630651994741574 else to node 7.
			node=6 is a leaf node.
			node=7 is a leaf node.
	node=8 is a split node: go to node 9 if X[:, 30] <= 1.455275263643501 else to node 12.
		node=9 is a split node: go to node 10 if X[:, 13] <= -0.9017924160395874 else to node 11.
			node=10 is a leaf node.
			node=11 is a leaf node.
		node=12 is a leaf node.
Best: 0.825000 using {'DT__criterion': 'gini', 'DT__max_depth': 5, 'DT__splitter': 'best'}
The binary tree structure has 11 nodes and has the following tree structure:

node=0 is a split node: go to node 1 if X[:, 27] <= 0.02871482726186514 else to node 4.
	node=1 is a split node: go to node 2 if X[:, 40] <= 1.7569937109947205 else to node 3.
		node=2 is a leaf node.
		node=3 is a leaf node.
	node=4 is a split node: go to node 5 if X[:, 36] <= -0.8872300088405609 else to node 8.
		node=5 is a split node: go to node 6 if X[:, 19] <= 0.6994592547416687 else to node 7.
			node=6 is a leaf node.
			node=7 is a leaf node.
		node=8 is a split node: go to node 9 if X[:, 37] <= -1.3058494329452515 else to node 10.
			node=9 is a leaf node.
			node=10 is a leaf node.
AUC = 0.662608695652174
-- LOGISTIC REGRESSION MODEL --
Number of experiments: 10
Best: 0.925000 using {'LOGR__C': 0.1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.01907221 -0.05717974  0.03886115  0.05310638 -0.05792491  0.01553325
   0.00668575 -0.02401791  0.01912994  0.0342286  -0.05851811  0.02428937
   0.04868838 -0.04479078 -0.01553418  0.02532775 -0.13591544  0.11404542
   0.04080005  0.04674724 -0.07207761 -0.29734509  0.1532319   0.22675224
   0.16631014 -0.12441463 -0.1003222  -0.2108467   0.21583756  0.04626065
  -0.05886805  0.02423139  0.06724372 -0.04316321  0.08127136 -0.04570787
  -0.16620027  0.15931358  0.04590148 -0.09130617  0.05109079  0.06152001
  -0.127853    0.25815718 -0.1613441   0.01928828  0.10159452 -0.17176402]]
Best: 1.000000 using {'LOGR__C': 10, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[-0.28252651  0.22285443  0.1193276   0.22561908 -0.36321598  0.16025277
  -0.33494189  0.30281812  0.13389407  0.1910117  -0.45699828  0.23337286
   0.38148538 -0.37231134 -0.09388225 -0.62396972  0.38360269  0.38224356
   0.07019026 -0.70625616  0.61490282 -2.12749023  1.11688933  1.60573041
  -0.51587296  0.58842493 -0.00747386 -1.12154959  1.26863074  0.07858781
   0.3411209  -0.49547414  0.15321151 -0.92799815  1.22599726 -0.17949836
  -0.51392139  0.52603734  0.047474   -0.87727688  0.93302259  0.1797346
  -1.43303939  1.90734592 -0.589537   -0.13324466  1.50750761 -1.7035465 ]]
Best: 0.883333 using {'LOGR__C': 1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.11840559 -0.18202691  0.05185801  0.06209241  0.15305774 -0.18354771
   0.14517053 -0.13091494 -0.04889032  0.12591571 -0.25288001  0.13755896
   0.01906479 -0.03418404  0.01418731  0.17407018 -0.30888412  0.14064003
   0.06897439 -0.40571603  0.32235773 -0.99770184  0.78401851  0.52259107
   0.23657019 -0.21636896 -0.09035746 -0.4900493   0.53376093  0.06096681
  -0.17857607  0.13451477  0.11512821 -0.27961622  0.39364005 -0.09002958
  -0.27179     0.28173884  0.03832559 -0.19652791  0.02956325  0.22874286
  -0.56180573  0.72328057 -0.21298232 -0.01322397  0.51318458 -0.61018807]]
Best: 0.966667 using {'LOGR__C': 0.1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.0158756  -0.07056635  0.05670806  0.0486698  -0.0323374  -0.00357081
   0.01258152 -0.05521006  0.04569079  0.02817437 -0.07533437  0.05026268
   0.04500268 -0.0814191   0.0334374   0.01555686 -0.10401961  0.09786074
   0.0249312   0.0343763  -0.05109679 -0.27352443  0.14746158  0.21527978
   0.147051   -0.10316064 -0.10795558 -0.20410652  0.21378129  0.03363894
  -0.07602751  0.0532669   0.05503674 -0.02992223  0.02652595  0.01158598
  -0.24069409  0.24185873  0.04172327 -0.13118628  0.08852791  0.07546089
  -0.14679647  0.23708933 -0.11207331  0.03837348  0.10529875 -0.18302875]]
Best: 0.866667 using {'LOGR__C': 0.1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.02140597 -0.07697316  0.06011315  0.05174145 -0.02622012 -0.00959334
   0.01790147 -0.05529626  0.0418703   0.02735519 -0.06221773  0.0329443
   0.04444043 -0.06085765  0.0123751   0.00412425 -0.0892412   0.10549737
  -0.00064488  0.11037202 -0.11732619 -0.26851736  0.10130718  0.21252833
   0.10797643 -0.02334658 -0.14144615 -0.23652637  0.22317218  0.0952357
  -0.05481848 -0.00916752  0.08808727 -0.05066992  0.06893821 -0.0156461
  -0.17326886  0.16240086  0.04841264 -0.10156987  0.0523998   0.08370696
  -0.11802444  0.2231695  -0.13715097  0.00844466  0.15984234 -0.20925831]]
Best: 0.916667 using {'LOGR__C': 0.1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.02748799 -0.07088179  0.04616306  0.04551016 -0.00660356 -0.02563249
   0.02917298 -0.04723808  0.0152783   0.03146031 -0.06784085  0.03907759
   0.06256129 -0.08897641  0.01976631 -0.01296645 -0.06321884  0.0873381
   0.01241457  0.04954024 -0.0562497  -0.26787837  0.13437137  0.2076519
   0.16913459 -0.12731171 -0.10673318 -0.20780421  0.21722375  0.03575356
  -0.08349957  0.06507243  0.04889777 -0.04909144  0.04913352  0.01265406
  -0.16058677  0.15774698  0.03683076 -0.11315825  0.0745383   0.07320034
  -0.17655687  0.29443042 -0.15004995  0.04230165  0.11459646 -0.20380547]]
Best: 0.916667 using {'LOGR__C': 1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.16504269 -0.20080245  0.00879372  0.06943902  0.08647306 -0.12621685
   0.18811261 -0.14576533 -0.08883775  0.13901735 -0.17885551  0.01900031
   0.09414168 -0.05560537 -0.06099827  0.19286455 -0.39137759  0.20768405
  -0.17071612 -0.06485038  0.18101734 -1.11759581  0.56569215  0.86566509
   0.14176471 -0.09574772 -0.10261264 -0.54831518  0.58098582  0.08809533
  -0.13651426  0.06886332  0.13701051 -0.26714886  0.38117246 -0.10943163
  -0.31109519  0.3226045   0.0456683  -0.20950576  0.03788554  0.22939865
  -0.53127474  0.64700252 -0.1338404  -0.05404044  0.56810806 -0.73289697]]
Best: 0.925000 using {'LOGR__C': 1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.25367314 -0.33219884  0.03319044  0.09274921 -0.06465528 -0.00521107
   0.23953245 -0.22366122 -0.07876663  0.10943674 -0.13217757  0.00466094
   0.12337297 -0.09597567 -0.05579741  0.13693894 -0.38174094  0.24809833
  -0.14107571  0.00242507  0.09580174 -1.15628197  0.7567838   0.76234613
   0.11880808 -0.02877917 -0.17608684 -0.55735314  0.57573895  0.10586292
  -0.21158932  0.15027979  0.1486522  -0.30179479  0.39442888 -0.06272478
  -0.23253869  0.23049668  0.05669604 -0.21651369  0.04678226  0.23460962
  -0.3515722   0.45269567 -0.12857228 -0.21688486  0.71587733 -0.60574618]]
Best: 0.916667 using {'LOGR__C': 3, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.33471832 -0.36108777 -0.04137638  0.05265957  0.28756473 -0.29874049
   0.41376795 -0.3341264  -0.2004275   0.18431064 -0.19905114 -0.01968643
   0.04814685  0.04891724 -0.12379457  0.42841177 -0.6890356   0.26185984
  -0.4298806  -0.23474471  0.5235827  -1.84367328  0.88482658  1.50158283
  -0.00861601  0.04609072 -0.0580091  -0.78059996  0.82791856  0.11503423
  -0.17155387  0.06745192  0.21565495 -0.55380553  0.73267436 -0.12358478
  -0.41524586  0.43819375  0.05196447 -0.3806622   0.08447359  0.41620443
  -0.95748848  0.95486747  0.00867376  0.00488278  1.02402081 -1.33145963]]
Best: 0.950000 using {'LOGR__C': 1, 'LOGR__penalty': 'l2'}
Coefficient of the features in the decision function:  [[ 0.15638315 -0.19133702  0.01423806  0.07057782  0.03527238 -0.07959728
   0.16550894 -0.13303207 -0.0778068   0.12911176 -0.16019366  0.02016215
   0.07392262 -0.02907554 -0.0674349   0.21154047 -0.41369987  0.21503774
  -0.1809304  -0.10638352  0.228033   -1.12749469  0.54425288  0.89671516
   0.10770538 -0.04574394 -0.12778969 -0.57833311  0.59563914  0.08183511
  -0.09947598  0.03295268  0.13227621 -0.24001956  0.34720681 -0.10407323
  -0.3211771   0.32672991  0.04249259 -0.23210463  0.10201373  0.20657788
  -0.60816642  0.75874233 -0.16928667 -0.06973621  0.57226648 -0.60811671]]
AUC = 0.8486956521739131
              SensLevel0          SensLevel1  ...  SpeCI_lo  SpeCI_hi
SVM   0.6521739130434783  0.8260869565217391  ...   0.69629   0.98371
KNN   0.4782608695652174  0.6521739130434783  ...   0.69629   0.98371
ADAB  0.6956521739130435  0.8695652173913043  ...  0.752615  1.007385
RF    0.6086956521739131  0.9130434782608695  ...    0.6432    0.9568
LGB   0.6521739130434783  0.8260869565217391  ...  0.592583  0.927417
XGB    0.782608695652174  0.9565217391304348  ...   0.69629   0.98371
DT    0.1304347826086956  0.6086956521739131  ...  0.497141  0.862859
LOGR  0.6086956521739131  0.8260869565217391  ...   0.69629   0.98371

[8 rows x 12 columns]
Elapsed time:  249.64582180976868